{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9d943c2-daf5-439d-b654-5f95839a91ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('full_meme_data.csv', names=['image', 'description', 'full_description', 'prois', 'meaning'], sep=',')[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5afeecc5-2af3-4366-9260-7024c1ed1ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = df['image'].tolist()\n",
    "captions = df['description'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9460f3ed-6bcd-4ab9-a242-49aa922b2b60",
   "metadata": {},
   "source": [
    "# Initialize models and download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68aa5fc1-bc25-484c-8809-b3c0218ee17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-125m\")\n",
    "language_model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-125m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5adb1372-8a2f-4e61-8031-75b6977cc598",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"bos_token_id\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"eos_token_id\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"bos_token_id\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"eos_token_id\"]` will be overriden.\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoProcessor, CLIPModel\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-large-patch14\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "069820ad-da5c-4a28-9b73-6b838ef23972",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.add_special_tokens({\"additional_special_tokens\": [\"<RET>\"]})\n",
    "\n",
    "language_model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "ret_token_id = tokenizer.encode(\"<RET>\", add_special_tokens=False)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dbfd1300-99f8-49fc-964e-4a3fa934150f",
   "metadata": {},
   "outputs": [],
   "source": [
    "language_model = language_model.eval()\n",
    "clip_model = clip_model.eval()\n",
    "\n",
    "for param in language_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for param in clip_model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0c49ba5-7580-4526-bab3-5e3916102ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import requests\n",
    "import urllib3\n",
    "\n",
    "\n",
    "def download_image(url: str) -> urllib3.response.HTTPResponse:\n",
    "    raw_image = url\n",
    "    if url.startswith('http'):\n",
    "        raw_image = requests.get(url, stream=True).raw\n",
    "    return raw_image\n",
    "\n",
    "\n",
    "def download_image_batch(image_urls):\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        futures = []\n",
    "        for url in image_urls:\n",
    "            futures.append(executor.submit(download_image, url))\n",
    "    \n",
    "        raw_images = []\n",
    "        for future in as_completed(futures):\n",
    "            raw_image = future.result()\n",
    "            if raw_image is not None:\n",
    "                raw_images.append(raw_image)\n",
    "    return raw_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cd231ab5-1143-456d-8699-3a37d02b851c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def return_image_embeddings(images):\n",
    "    with torch.no_grad():\n",
    "        inputs = processor(images=images, return_tensors=\"pt\", padding=True).to(clip_model.device)\n",
    "        embeds = clip_model.get_image_features(**inputs)\n",
    "    return embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3fd6496d-f929-4c3a-84c5-3d54bb9c21b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 16/16 [01:18<00:00,  4.90s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "total_embeds = []\n",
    "\n",
    "for image_batch in tqdm(np.array_split(images, len(images) // 128)):\n",
    "    raw_images = download_image_batch(image_batch)\n",
    "    opened_images = [Image.open(raw_image) for raw_image in raw_images]\n",
    "    embeds = return_image_embeddings(opened_images)\n",
    "    total_embeds.append(embeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7313aa41-968d-4c75-9614-1f1186f29141",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_embeds = torch.cat(total_embeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "143d8a88-9a32-41e2-8754-38bab144f89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !mkdir fromage_weights\n",
    "torch.save(total_embeds, 'fromage_weights/images_embeds.pt')\n",
    "\n",
    "# total_embeds = torch.load('fromage_weights/images_embeds.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dfc800e2-c423-4414-a35d-ca763cda7178",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2124, 768])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_embeds.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3afd79c7-93d3-4a0b-9e92-1bc32c67a840",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "df1a8773-1b43-4c09-bfdf-cee6514a1e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class MultiModalDataset(Dataset):\n",
    "    def __init__(self, input_ids, image_embeddings):\n",
    "        self.input_ids = input_ids\n",
    "        self.image_embeddings = image_embeddings\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return {'input_ids': self.input_ids[index], \n",
    "                'image_embeddings': self.image_embeddings[index],\n",
    "                'labels': torch.clone(self.input_ids[index]),\n",
    "               }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "70c2bcf3-a787-4739-9d3c-64d113cdfcc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = [text + \"<RET>\" for text in captions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "0e5ed318-6e76-4d4c-b544-e93efd42c879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "212\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1912, 212)"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = tokenizer(input_text, return_tensors='pt', padding=True)['input_ids']\n",
    "\n",
    "threshold = int(0.1 * len(input_ids))\n",
    "\n",
    "print(threshold)\n",
    "\n",
    "train_dataset = MultiModalDataset(input_ids[:-threshold], total_embeds[:-threshold])\n",
    "val_dataset = MultiModalDataset(input_ids[-threshold:], total_embeds[-threshold:])\n",
    "\n",
    "len(train_dataset), len(val_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5820d634-2a54-43a3-9e2e-f0d5a81a93f2",
   "metadata": {},
   "source": [
    "# Fromage retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "3445d5bc-8261-4444-902c-6a14f6f7f118",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class FromageRetrieval(torch.nn.Module):\n",
    "    def __init__(self, language_model, clip_model, ret_token_id: int):\n",
    "        super().__init__()\n",
    "        self.language_model = language_model\n",
    "        self.clip_model = clip_model\n",
    "\n",
    "        self.adapter_dim = 512\n",
    "        self.token_emb_dim = language_model.config.word_embed_proj_dim\n",
    "        self.visual_emb_dim = clip_model.config.projection_dim\n",
    "\n",
    "        self.image_projection = torch.nn.Linear(self.visual_emb_dim, self.adapter_dim)\n",
    "        self.text_projection = torch.nn.Linear(self.token_emb_dim, self.adapter_dim)\n",
    "\n",
    "        self.ret_token_id = ret_token_id\n",
    "\n",
    "    def forward(\n",
    "        self, \n",
    "        input_ids: torch.tensor, \n",
    "        image_embeddings: torch.Tensor, \n",
    "        attention_mask: torch.Tensor,\n",
    "        labels: torch.Tensor,\n",
    "    ):\n",
    "        lm_out = self.language_model(\n",
    "            input_ids=input_ids, \n",
    "            labels=labels, \n",
    "            attention_mask=attention_mask, \n",
    "            output_hidden_states=True\n",
    "        )\n",
    "\n",
    "        ret_token_indicies = torch.nonzero(input_ids == self.ret_token_id)[:, 1]\n",
    "\n",
    "        hidden_state = lm_out.hidden_states[-1]\n",
    "\n",
    "        ret_embs = torch.gather(hidden_state, 1, ret_token_indicies.unsqueeze(1).unsqueeze(2).repeat(1, 1, hidden_state.shape[-1]))\n",
    "\n",
    "        projected_ret_embs = self.text_projection(ret_embs)\n",
    "        projected_image_embs = self.image_projection(image_embeddings)\n",
    "\n",
    "        return lm_out, projected_ret_embs, projected_image_embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "97572988-3c21-4186-8b61-e2b5912a770b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fromage_retrieval_model = FromageRetrieval(language_model=language_model, clip_model=clip_model, ret_token_id=ret_token_id)\n",
    "\n",
    "# string = [\"Hello world! <RET>\", \n",
    "#           \"my name is <RET>\", \n",
    "#           \"i <RET>\"]\n",
    "\n",
    "# input_ids = tokenizer(string, return_tensors='pt', padding=True)['input_ids']\n",
    "\n",
    "# image_embeddings = torch.rand(3, 768)\n",
    "\n",
    "# lm_out, projected_ret_embs, projected_image_embs = fromage_retrieval_model(input_ids=input_ids, image_embeddings=image_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5081fc-11c9-4bd5-ad9f-d3d368922453",
   "metadata": {},
   "source": [
    "# Custom Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd61c448-4a67-4741-bb3d-f95e6088400f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "1e1c48d9-075a-40a2-9906-1055d4912b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "from info_nce_loss import InfoNCE\n",
    "\n",
    "class CustomFromageTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        lm_out, projected_ret_embs, projected_image_embs = model(**inputs)\n",
    "        cross_entropy_loss = lm_out.loss\n",
    "\n",
    "        info_nce_loss_criterion = InfoNCE()\n",
    "\n",
    "        projected_ret_embs = projected_ret_embs.squeeze(1)\n",
    "        \n",
    "        # print(projected_ret_embs.shape, projected_image_embs.shape)\n",
    "\n",
    "        info_nce_loss = info_nce_loss_criterion(projected_ret_embs, projected_image_embs, None)\n",
    "\n",
    "        loss = cross_entropy_loss + info_nce_loss\n",
    "        return (loss, lm_out) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "b971eff4-0dbc-4ffa-977e-0202249010ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.rand(5, 1, 10).squeeze(1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "535b34da-7d20-4568-9966-d63ab545e6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !export WANDB_MODE=disable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "47923ee2-6b3e-45e5-bcd3-9de320a75fa4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='150' max='150' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [150/150 01:15, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>11.165600</td>\n",
       "      <td>11.230010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128</td>\n",
       "      <td>11.130600</td>\n",
       "      <td>11.229642</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=150, training_loss=11.144424845377603, metrics={'train_runtime': 77.0552, 'train_samples_per_second': 124.067, 'train_steps_per_second': 1.947, 'total_flos': 0.0, 'train_loss': 11.144424845377603, 'epoch': 5.0})"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "fromage_retrieval_model = FromageRetrieval(language_model=language_model, clip_model=clip_model, ret_token_id=ret_token_id)\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "    dataloader_pin_memory=False,\n",
    "    output_dir='.',\n",
    "    num_train_epochs=5,\n",
    "    logging_strategy='steps',\n",
    "    logging_steps=64,\n",
    "    eval_steps=64,\n",
    "    do_eval=True,\n",
    "    evaluation_strategy='steps',\n",
    ")\n",
    "\n",
    "custom_fromage_trainer = CustomFromageTrainer(\n",
    "    model=fromage_retrieval_model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    args=training_arguments,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "custom_fromage_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "b1ebeeb5-8817-4138-b94b-85806b0122b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_fromage_trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942f112a-6984-4084-9399-e02bc68590d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
